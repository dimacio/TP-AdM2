version: '3.8'

networks:
  ml_pipeline_net:
    name: ml_pipeline_net
    driver: bridge

volumes:
  postgres_mlflow_data:
  minio_data:
  postgres_airflow_data:
  dags_data:
  logs_data:

services:
  # --- Infraestructura MLflow ---
  postgres_mlflow:
    image: postgres:13
    container_name: mlflow_postgres_db
    hostname: postgres_mlflow
    restart: always
    networks:
      - ml_pipeline_net
    environment:
      POSTGRES_USER: ${PG_USER:-mlflow}
      POSTGRES_PASSWORD: ${PG_PASSWORD:-mlflow}
      POSTGRES_DB: ${PG_DATABASE:-mlflow}
    volumes:
      - postgres_mlflow_data:/var/lib/postgresql/data

  s3:
    image: minio/minio:latest
    container_name: mlflow_minio_s3
    hostname: s3
    restart: always
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - ml_pipeline_net
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data

  create_s3_bucket:
    image: minio/mc
    container_name: mlflow_mc_bucket_creator
    depends_on: [s3]
    networks:
      - ml_pipeline_net
    entrypoint: >
      /bin/sh -c '
        until mc alias set s3alias http://s3:9000 ${MINIO_ROOT_USER:-minioadmin} ${MINIO_ROOT_PASSWORD:-minioadmin}; do sleep 2; done;
        BUCKET_NAME=$${MLFLOW_BUCKET_NAME:-mi-bucket-experimentos};
        if ! mc ls "s3alias/$${BUCKET_NAME}" > /dev/null 2>&1; then
          mc mb "s3alias/$${BUCKET_NAME}";
          mc policy set public "s3alias/$${BUCKET_NAME}";
        else
          echo "Bucket $${BUCKET_NAME} already exists.";
        fi;
      '
  
  mlflow:
    build:
      context: .
      dockerfile: ./dockerfiles/Dockerfile.mlflow
    image: mlflow_server_custom
    container_name: mlflow_server_instance
    hostname: mlflow
    restart: always
    depends_on: [postgres_mlflow, create_s3_bucket]
    ports: ["5001:5000"]
    networks:
      - ml_pipeline_net
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      MLFLOW_S3_ENDPOINT_URL: http://s3:9000
    command: >
      mlflow server
      --backend-store-uri postgresql://${PG_USER:-mlflow}:${PG_PASSWORD:-mlflow}@postgres_mlflow:5432/${PG_DATABASE:-mlflow}
      --default-artifact-root s3://${MLFLOW_BUCKET_NAME:-mlflow-bucket}/
      --host 0.0.0.0
      --port 5000

  # --- Infraestructura Airflow ---
  postgres_airflow:
    image: postgres:13
    container_name: airflow_postgres_db
    hostname: postgres_airflow
    restart: always
    networks:
      - ml_pipeline_net
    environment:
      POSTGRES_USER: ${AIRFLOW_DB_USER:-airflow}
      POSTGRES_PASSWORD: ${AIRFLOW_DB_PASSWORD:-airflow}
      POSTGRES_DB: ${AIRFLOW_DB_NAME:-airflow}
    volumes:
      - postgres_airflow_data:/var/lib/postgresql/data

  airflow-init:
    build:
      context: .
      dockerfile: ./dockerfiles/Dockerfile.airflow
    image: custom_airflow_image
    container_name: airflow_init
    depends_on: [postgres_airflow]
    networks:
      - ml_pipeline_net
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER:-airflow}:${AIRFLOW_DB_PASSWORD:-airflow}@postgres_airflow:5432/${AIRFLOW_DB_NAME:-airflow}
    entrypoint: /bin/bash
    command: -c 'airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com'

  airflow-webserver:
    build:
      context: .
      dockerfile: ./dockerfiles/Dockerfile.airflow
    image: custom_airflow_image
    container_name: airflow_webserver
    restart: always
    depends_on: [airflow-init]
    ports: ["8080:8080"]
    networks:
      - ml_pipeline_net
    volumes:
      - ./dags:/opt/airflow/dags
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER:-airflow}:${AIRFLOW_DB_PASSWORD:-airflow}@postgres_airflow:5432/${AIRFLOW_DB_NAME:-airflow}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY:-a_truly_secret_key}
      MLFLOW_TRACKING_URI: 'http://mlflow:5000'
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      MLFLOW_S3_ENDPOINT_URL: 'http://s3:9000'
    command: bash -c "exec /entrypoint webserver" # <-- CAMBIO CLAVE

  airflow-scheduler:
    build:
      context: .
      dockerfile: ./dockerfiles/Dockerfile.airflow
    image: custom_airflow_image
    container_name: airflow_scheduler
    restart: always
    depends_on: [airflow-init]
    networks:
      - ml_pipeline_net
    volumes:
      - ./dags:/opt/airflow/dags
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER:-airflow}:${AIRFLOW_DB_PASSWORD:-airflow}@postgres_airflow:5432/${AIRFLOW_DB_NAME:-airflow}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY:-a_truly_secret_key}
      MLFLOW_TRACKING_URI: 'http://mlflow:5000'
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      MLFLOW_S3_ENDPOINT_URL: 'http://s3:9000'
    command: bash -c "exec /entrypoint scheduler" # <-- CAMBIO CLAVE

# --- Infraestructura FastAPI ---
  api:
    build:
      context: ./api
      dockerfile: ../dockerfiles/Dockerfile.fastapi
    image: stock_predictor_api
    container_name: stock_predictor_api
    restart: always
    ports:
      - "8000:8000"
    networks:
      - ml_pipeline_net
    # environment:
      # Add environment variables if needed